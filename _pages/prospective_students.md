---
layout: page
permalink: /prospective_students/
inline: true
title: prospective
related_posts: false
nav: false
---

Thank you for your interest in my research group!

<p class="important-announcement">Prospective Ph.D. Students</p> 
 I am currently looking for motivated students. Please apply through the [Rutgers Ph.D. Program in Computer Science](https://www.cs.rutgers.edu/academics/graduate/prospective-students/) and mention my name in your application. The Ph.D. application deadline for students starting in Fall 2026 is January 1, 2026. Feel free to send me a short email with a brief description of your research interests and experience. Be sure to include your CV and transcript, and add “Prospective Ph.D. Student” to the subject line. 
 
<p class="important-announcement">I am especially looking for students to work on the following topics:</p> 
<ul>
<li> Diversity/Personalization/Uncertainty in LLMs - Developing methods that allow large language models to produce diverse yet valid answers. Such diversity reflects natural human reasoning and supports personalization, alignment, and uncertainty estimation. It can arise from different reasoning paths, internal representations, or parameter configurations, and connects directly to the Rashomon Effect (the existence of many equally good but behaviorally different solutions).
<li> Interpretable AI - Understanding how large language models organize information across layers, neurons, and attention heads. This includes studying sparse or modular circuits, identifying functional subcomponents, and developing transformer-specific interpretability tools. A key challenge is evaluation, as in determining when an explanation is faithful and stable, and understanding of how explanations differ across equally good model variants, revealing the multiplicity inside LLMs.
<li> Stability in LLMs and Foundation Models - Examining how sensitive LLMs are to small changes in prompts, data, fine-tuning, or random seeds. Even models with identical performance can vary internally across training runs. My goal is to identify which aspects of model behavior are stable and reproducible, and which ones vary across alternative solutions, a perspective that aligns naturally with the Rashomon Effect.
<li> Interpretable ML and Theory-Driven ML - Designing interpretable machine learning algorithms and studying practical ML phenomena from a theoretical perspective. This includes understanding when simple models can match the performance of black-box systems, and developing interpretable methods for high-stakes domains such as medical and financial data. This line of work builds on my prior research on the Rashomon Effect and interpretable ML, and you can read my publications for more details on these topics.
</ul>

<p class="important-announcement">Rutgers Undergraduate or Graduate Students</p> 
Please visit the [Research page]({{ site.baseurl }}/research/) to learn more about my projects.  If you are interested in collaborating, send me an email describing your background and what you would like to work on. Include your CV and transcript, indicate how much time you can dedicate to the project, and add “Rutgers Student Collaborator” to the email subject line.

<p class="important-announcement">Visiting Student Researchers or Interns</p> 
We have various projects that would benefit from collaboration with interns or visiting students who have a strong background in machine learning, Python programming, and/or optimization, machine learning theory, human-computer interaction, mechanistic interpretability, or foundation models. If you are interested in working together, send me a brief email outlining your research interests and experience. Include your CV, a note on how much time you can dedicate to the project, and add “Visiting Researcher/Intern” to the email subject line. I may have limited bandwidth, but I will do my best to reply to your message.

<!-- Diversity/Personalization/Uncertainty in LLMs - Developing methods that allow large language models to produce diverse, yet valid, answers. Such diversity is natural in human reasoning and closely tied to personalization (adapting outputs to individual preferences), alignment (reflecting a range of acceptable viewpoints), and uncertainty estimation (understanding where multiple plausible outputs exist). This diversity can arise from different reasoning paths, different internal representations, or different parameter configurations, and connects directly to the Rashomon Effect—the existence of many equally good but behaviorally different solutions. Interpretable AI - Understanding the internal structure of large language models, especially how transformers organize information across layers, neurons, and attention heads. This includes studying sparse or modular circuits, discovering functional subcomponents, and developing transformer-specific interpretability methods that trace how information flows through the model. A central challenge in this area is evaluation (determining when an explanation is faithful, stable, and not an artifact of the probing method). I am also interested in how interpretability methods vary across multiple equally good model variants, where differences in internal structure reveal the multiplicity underlying LLMs. Stability in LLMs and Foundation Models - Studying the stability of large language models, including how sensitive their predictions, reasoning, and internal representations are to small changes in prompts, training data, fine-tuning procedures, or random seeds. Foundation models often exhibit significant variation across different training runs or parameter initializations, even when their overall performance is the same. I aim to understand which aspects of model behavior are stable and reproducible, and which ones vary across alternative but equally good solutions. This perspective naturally connects to the Rashomon Effect. Interpretable Model Design and Theory-Driven ML - Designing interpretable machine learning algorithms and studying practical ML phenomena from a theoretical perspective. This includes understanding when simple or structured models can match the performance of more complex systems, and developing sparse methods tailored to high-dimensional biomedical or clinical data. This line of work builds on my prior research on the Rashomon Effect and interpretable ML and you can read my publications for more details on these topics.
<p class="important-announcement">Some of the topics that we are currently working on or plan to work on include:</p> 
<ul>
<li>Quantifying and measuring model multiplicity/uncertainty in machine learning</li>
<li>Studying properties of the set of models that perform approximately equally well</li>
<li>Learning under noise</li>
<li>Robustness, distribution shifts, and dataset shifts</li>
<li>Designing interpretable machine learning models</li>
<li>Improving fairness and safety of machine learning models and data science pipelines</li>
<li>Evaluating trustworthiness and uncertainty of LLMs</li>
<li>Interpretability in reinforcement learning</li>
<li>Applications of machine learning in high-stakes decision domains (such as healthcare, finance, criminal justice, and governance), focusing on responsible AI/ML</li>
</ul>
 in Python, and/or expertise in at least one of the following areas: optimization, machine learning theory, deep learning, AI for healthcare, data science, or human-computer interaction
<p class="important-announcement"> Rutgers Students</p>
 If you are currently an undergraduate or graduate student at Rutgers, I would be more than happy to chat with you about research. Please take a look at the [Research page]({{ site.baseurl }}/research/), skim through the papers or paper abstracts that you find interesting, and send me a brief note. Include your CV and transcript in the email, and use the email subject line “[Rutgers Collaboration] your name.” -->
